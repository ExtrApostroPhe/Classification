{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ExtrApostroPhe/Summer_Project/blob/main/TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "933e192d",
      "metadata": {
        "id": "933e192d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer  \n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans \n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input, Dropout\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "o03EhlfEDdr7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o03EhlfEDdr7",
        "outputId": "b8f1b0e4-2600-4091-9f8d-2f03ad739953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stw = set(stopwords.words('english'))\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)  \n",
        "os.chdir('/content/drive/My Drive/news_ipm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "395c76b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "395c76b9",
        "outputId": "4042193b-d6a8-4410-aad3-451ba16bfa8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                       0  1\n",
            "0          'Chief' system boosts water quality of rivers  0\n",
            "1                    County tackles soil erosion head-on  0\n",
            "2         30% cut expected in new high-emission projects  0\n",
            "3             High-altitude wind farm in Tibet starts up  0\n",
            "4               System proves Fuzhou's green credentials  0\n",
            "...                                                  ... ..\n",
            "23861    Ministry: 'Genocide in Xinjiang' another US lie  6\n",
            "23862             Yunnan city reports new COVID-19 cases  6\n",
            "23863  Judges urge trademark applicants to be more ho...  6\n",
            "23864                  Cherry trees of a different color  6\n",
            "23865  New book on Party history released at Beijing ...  6\n",
            "\n",
            "[23866 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Read Dataset\n",
        "data_env = pd.read_csv('./train_data/environment.txt', header = None, sep = \"\\n\")  # 这里只获取部分数据\n",
        "data_health = pd.read_csv('./train_data/health.txt', header = None, sep = \"\\n\")\n",
        "data_hk = pd.read_csv('./train_data/hk_macao.txt', header = None, sep = \"\\n\")\n",
        "data_inno = pd.read_csv('./train_data/innovation.txt', header = None, sep = \"\\n\")\n",
        "data_military = pd.read_csv('./train_data/military.txt', header = None, sep = \"\\n\")\n",
        "data_notional = pd.read_csv('./train_data/notional_affairs.txt', header = None, sep = \"\\n\")\n",
        "data_society = pd.read_csv('./train_data/society.txt', header = None, sep = \"\\n\")\n",
        "\n",
        "data_env[1] = \"0\"\n",
        "data_health[1] = \"1\"\n",
        "data_hk[1] = \"2\"\n",
        "data_inno[1] = \"3\"\n",
        "data_military[1] = \"4\"\n",
        "data_notional[1] = \"5\"\n",
        "data_society[1] = \"6\"\n",
        "\n",
        "# print(type(data))\n",
        "# print(data_env, data_health)\n",
        "# print(data_health)\n",
        "data = pd.concat([data_env, data_health, data_hk, data_inno, data_military, data_notional, data_society ], axis = 0, ignore_index = True)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5459bf95",
      "metadata": {
        "id": "5459bf95"
      },
      "outputs": [],
      "source": [
        "X = data[0]\n",
        "Y = data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5118858f",
      "metadata": {
        "id": "5118858f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b38923d-b168-4a15-ef0c-6c85c4d6ac29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stw,max_df=0.5)\n",
        "X = tfidf.fit_transform(X).toarray()\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "94b94f35",
      "metadata": {
        "id": "94b94f35"
      },
      "outputs": [],
      "source": [
        "def insert_text():\n",
        "    X_temp = []\n",
        "    for x in X:\n",
        "        X_temp.append(x)\n",
        "    return X_temp\n",
        "\n",
        "def filter_text(text):\n",
        "    str = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    return re.sub(r'\\s+', ' ', str)\n",
        "\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def lemmatization_text(text_cut_list):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(n) for n in text_cut_list]\n",
        "\n",
        "def text_cut(text):\n",
        "    return re.findall('[a-zA-z]+', text)\n",
        "\n",
        "def stopwords_text(text_cut_list):\n",
        "    Stop_Word_list = stopwords.words(\"english\")\n",
        "    return [n for n in text_cut_list if n not in Stop_Word_list]\n",
        "\n",
        "article = insert_text()\n",
        "X = []\n",
        "# print(article)\n",
        "for content in article:\n",
        "    content = filter_text(content)\n",
        "    content = lower_text(content)\n",
        "#     print(content)\n",
        "    word_cut = lemmatization_text(text_cut(content))\n",
        "    word_cut = stopwords_text(word_cut)\n",
        "#     print(word_cut)\n",
        "    content = ''\n",
        "    for w in word_cut:\n",
        "        content = content + w + ' '\n",
        "    X.append(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "52f412fd",
      "metadata": {
        "id": "52f412fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de38b136-fc1c-470c-ce99-801a30b85927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "多项式朴素贝叶斯文本分类的准确率为： 0.6606145251396648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.67      0.66       845\n",
            "           1       0.69      0.71      0.70      1192\n",
            "           2       0.75      0.74      0.75      1175\n",
            "           3       0.76      0.73      0.74      1250\n",
            "           4       0.56      0.69      0.62       240\n",
            "           5       0.64      0.59      0.61      1312\n",
            "           6       0.50      0.51      0.51      1146\n",
            "\n",
            "    accuracy                           0.66      7160\n",
            "   macro avg       0.65      0.66      0.66      7160\n",
            "weighted avg       0.66      0.66      0.66      7160\n",
            "\n",
            "[[569  18  13  51   4  74 116]\n",
            " [ 15 852  71  66   7  64 117]\n",
            " [ 19  64 875  27  16 106  68]\n",
            " [ 53  89  36 908  17  49  98]\n",
            " [  4   5   9  22 165  22  13]\n",
            " [ 85  74  86  45  65 771 186]\n",
            " [126 128  80  83  19 120 590]]\n"
          ]
        }
      ],
      "source": [
        "nb_model = MultinomialNB(alpha=0.001)\n",
        "nb_model.fit(x_train,y_train)\n",
        "predict_test = nb_model.predict(x_test)\n",
        "print(\"多项式朴素贝叶斯文本分类的准确率为：\",metrics.accuracy_score(predict_test,y_test))\n",
        "print(metrics.classification_report(predict_test, y_test))\n",
        "print(metrics.confusion_matrix(predict_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ae813def",
      "metadata": {
        "id": "ae813def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5134b0-a339-4871-ea4e-f9e6e0fd397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bernoulli贝叶斯文本分类的准确率为： 0.6804469273743017\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.68       879\n",
            "           1       0.72      0.73      0.72      1220\n",
            "           2       0.78      0.80      0.79      1139\n",
            "           3       0.75      0.74      0.75      1228\n",
            "           4       0.63      0.66      0.65       280\n",
            "           5       0.65      0.61      0.63      1279\n",
            "           6       0.51      0.53      0.52      1135\n",
            "\n",
            "    accuracy                           0.68      7160\n",
            "   macro avg       0.68      0.68      0.68      7160\n",
            "weighted avg       0.68      0.68      0.68      7160\n",
            "\n",
            "[[592  17  11  55   4  79 121]\n",
            " [ 11 886  64  69   7  63 120]\n",
            " [ 10  43 915  20  13  84  54]\n",
            " [ 53  86  33 906  15  46  89]\n",
            " [  4   6  14  24 185  30  17]\n",
            " [ 80  73  68  43  52 782 181]\n",
            " [121 119  65  85  17 122 606]]\n"
          ]
        }
      ],
      "source": [
        "#bernoulli朴素贝叶斯\n",
        "ber_model = BernoulliNB(alpha=0.001)\n",
        "ber_model.fit(x_train,y_train)\n",
        "ber_predict = ber_model.predict(x_test)\n",
        "print(\"bernoulli贝叶斯文本分类的准确率为：\",metrics.accuracy_score(ber_predict,y_test))\n",
        "print(metrics.classification_report(ber_predict, y_test))\n",
        "print(metrics.confusion_matrix(ber_predict, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "031a1a0d",
      "metadata": {
        "id": "031a1a0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec544d09-9ff8-4285-83b7-866ea239539b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB贝叶斯文本分类的准确率为： 0.6804469273743017\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.68       879\n",
            "           1       0.72      0.73      0.72      1220\n",
            "           2       0.78      0.80      0.79      1139\n",
            "           3       0.75      0.74      0.75      1228\n",
            "           4       0.63      0.66      0.65       280\n",
            "           5       0.65      0.61      0.63      1279\n",
            "           6       0.51      0.53      0.52      1135\n",
            "\n",
            "    accuracy                           0.68      7160\n",
            "   macro avg       0.68      0.68      0.68      7160\n",
            "weighted avg       0.68      0.68      0.68      7160\n",
            "\n",
            "[[592  17  11  55   4  79 121]\n",
            " [ 11 886  64  69   7  63 120]\n",
            " [ 10  43 915  20  13  84  54]\n",
            " [ 53  86  33 906  15  46  89]\n",
            " [  4   6  14  24 185  30  17]\n",
            " [ 80  73  68  43  52 782 181]\n",
            " [121 119  65  85  17 122 606]]\n"
          ]
        }
      ],
      "source": [
        "gauss_model = GaussianNB()\n",
        "gauss_model.fit(x_train,y_train)\n",
        "gauss_predict = ber_model.predict(x_test)\n",
        "print(\"GaussianNB贝叶斯文本分类的准确率为：\",metrics.accuracy_score(gauss_predict,y_test))\n",
        "print(metrics.classification_report(gauss_predict, y_test))\n",
        "print(metrics.confusion_matrix(gauss_predict, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1178bb75",
      "metadata": {
        "id": "1178bb75"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4ca7b175",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ca7b175",
        "outputId": "05775eb1-66e6-4815-e69d-15b1488835e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "131/131 [==============================] - 16s 117ms/step - loss: 1.2128 - accuracy: 0.5766 - val_loss: 0.7428 - val_accuracy: 0.7575\n",
            "Epoch 2/10\n",
            "131/131 [==============================] - 15s 115ms/step - loss: 0.5006 - accuracy: 0.8368 - val_loss: 0.7348 - val_accuracy: 0.7566\n",
            "Epoch 3/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.2963 - accuracy: 0.9009 - val_loss: 0.8366 - val_accuracy: 0.7332\n",
            "Epoch 4/10\n",
            "131/131 [==============================] - 14s 110ms/step - loss: 0.1909 - accuracy: 0.9361 - val_loss: 0.9500 - val_accuracy: 0.7244\n",
            "Epoch 5/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.1365 - accuracy: 0.9557 - val_loss: 1.0545 - val_accuracy: 0.7173\n",
            "Epoch 6/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.1073 - accuracy: 0.9638 - val_loss: 1.1538 - val_accuracy: 0.7089\n",
            "Epoch 7/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.0998 - accuracy: 0.9662 - val_loss: 1.1820 - val_accuracy: 0.7098\n",
            "Epoch 8/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.0903 - accuracy: 0.9701 - val_loss: 1.2373 - val_accuracy: 0.7092\n",
            "Epoch 9/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.0789 - accuracy: 0.9735 - val_loss: 1.2692 - val_accuracy: 0.7092\n",
            "Epoch 10/10\n",
            "131/131 [==============================] - 14s 109ms/step - loss: 0.0783 - accuracy: 0.9749 - val_loss: 1.2840 - val_accuracy: 0.7007\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7674882e90>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#构建CNN分类模型(LeNet-5)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, input_shape = (x_train.shape[1], ), activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation = 'relu'))\n",
        "model.add(Dense(7, activation = 'softmax'))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=2)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data = (x_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TFIDF_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}